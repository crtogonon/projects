{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from math import isnan\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Part 3: Progress Report and Preliminary Findings\n",
    "\n",
    "This progress report will describe the process of working through this capstone project so far, including the successes and setbacks encountered along the way. Then I will run a preliminary model and discuss the initial results, as well as possible future steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Data\n",
    "\n",
    "For my project I needed to collect two different sets of data. I needed a list of players entering the 2017 NBA draft as well as their corresponding college career statistics, and I also needed the college statistics for NBA players, as well as their PER for their professional careers.\n",
    "\n",
    "The plan is to use the NBA player data as the training set for my model/models. The training data will be the college stats of the NBA players, with the target being the NBA players' NBA PERs. After fitting the model, on this training set, I will use the stats of the college players to predict their NBA career PER.\n",
    "\n",
    "I scraped an initial list of the draft prospects from draftexpress.com. For those players, I scraped basketball-reference.com for their college career statistics. I used the same website to scrape the statistics for a set of NBA players. The set of players includes active and non active US born players who went to college and played in the 3 point era (1979/80 through the present). Also, it includes only players who have played more than 82 games, which is the length of one season.\n",
    "\n",
    "It took some time to get the code right for my initial scrapes, but I was able to get the information that I needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning / Mining\n",
    "\n",
    "I decided to start by cleaning and mining my training data. The cleaning wasn't much of a problem. Since most columns were numbers, I just needed to make sure they were in the correct format. Also, I needed to make dummy variables for the player positions and the most frequently occuring schools. I also needed to insert NaNs into the dataframe where I needed to since there were missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing values\n",
    "\n",
    "This is where I ran into some trouble. The college player data was pretty much complete so there isn't much to worry about there. The NBA player data, however, contained a lot of missing values. There were a few columns that I felt I could drop entirely, but some I felt were too important to take out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('/Users/ct/DSI-NYC-4/projects/capstone/part-02/nba_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PER             0\n",
       "name            0\n",
       "g               0\n",
       "mp_per_g        0\n",
       "fg_per_g        0\n",
       "fga_per_g       0\n",
       "fg_pct          0\n",
       "fg2_per_g     425\n",
       "fg2a_per_g    448\n",
       "fg2_pct       448\n",
       "fg3_per_g     425\n",
       "fg3a_per_g    448\n",
       "fg3_pct       490\n",
       "ft_per_g        0\n",
       "fta_per_g       0\n",
       "ft_pct          0\n",
       "trb_per_g       0\n",
       "ast_per_g     320\n",
       "stl_per_g     320\n",
       "blk_per_g     320\n",
       "tov_per_g     484\n",
       "pf_per_g      303\n",
       "pts_per_g       0\n",
       "position        0\n",
       "school          0\n",
       "height          0\n",
       "weight          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the list of null values for my columns. There are a lot of missing values for stats like assists, blocks, steals, turnovers, and personal fouls. I don't want to drop those columns since I feel like they are important aspects of assessing player performance.\n",
    "\n",
    "I decided to try imputing missing values by using the NBA players' early professional career stats (first 3 years). I wanted to scrape first 3 year stats for NBA players, but the list generated for this on basketball reference was not identical to the list in the original scrape. It appears that there is no way to get the same list of players with my desired subset of stats.\n",
    "\n",
    "I scraped the list data I found from basketball reference and then tried to join it to the original NBA player dataframe on the player names. The problem was that some players have identical names. To fix this I manually made all of the names unique and was able to properly join the two dataframes. I then imputed the values from the bottom five columns with missing values using the newly scraped early career stats. This unfortunately did not completely take care of my NaNs. After imputation of those columns, they still had about 100 missing values each.\n",
    "\n",
    "In order to get to a state where I could run some sort of model, I decided to just drop the columns relating to 2 point and 3 point field goals and then drop rows still containing NaNs in the imputed columns. This dropped the numbers of rows from 1444 to 1304. I lost some data but I had no more NaNs in my data set and could start building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PER</th>\n",
       "      <th>name</th>\n",
       "      <th>g</th>\n",
       "      <th>mp_per_g</th>\n",
       "      <th>fg_per_g</th>\n",
       "      <th>fga_per_g</th>\n",
       "      <th>fg_pct</th>\n",
       "      <th>ft_per_g</th>\n",
       "      <th>fta_per_g</th>\n",
       "      <th>ft_pct</th>\n",
       "      <th>...</th>\n",
       "      <th>Alabama</th>\n",
       "      <th>Georgetown</th>\n",
       "      <th>Syracuse</th>\n",
       "      <th>Arkansas</th>\n",
       "      <th>Memphis</th>\n",
       "      <th>Louisville</th>\n",
       "      <th>Indiana</th>\n",
       "      <th>LSU</th>\n",
       "      <th>Notre Dame</th>\n",
       "      <th>Ohio State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.9</td>\n",
       "      <td>Michael Jordan</td>\n",
       "      <td>101</td>\n",
       "      <td>29.599117</td>\n",
       "      <td>7.1</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0.540</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.748</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.6</td>\n",
       "      <td>Anthony Davis</td>\n",
       "      <td>40</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.623</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.709</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.4</td>\n",
       "      <td>Shaquille O'Neal</td>\n",
       "      <td>90</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>8.7</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0.610</td>\n",
       "      <td>4.1</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.575</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.2</td>\n",
       "      <td>David Robinson</td>\n",
       "      <td>127</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>8.1</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0.613</td>\n",
       "      <td>4.8</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.7</td>\n",
       "      <td>Chris Paul</td>\n",
       "      <td>63</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>4.4</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.472</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.838</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    PER              name    g   mp_per_g  fg_per_g  fga_per_g  fg_pct  \\\n",
       "0  27.9    Michael Jordan  101  29.599117       7.1       13.2   0.540   \n",
       "1  26.6     Anthony Davis   40  32.000000       5.3        8.4   0.623   \n",
       "2  26.4  Shaquille O'Neal   90  30.500000       8.7       14.3   0.610   \n",
       "3  26.2    David Robinson  127  29.500000       8.1       13.3   0.613   \n",
       "4  25.7        Chris Paul   63  33.500000       4.4        9.3   0.472   \n",
       "\n",
       "   ft_per_g  fta_per_g  ft_pct     ...      Alabama  Georgetown  Syracuse  \\\n",
       "0       3.1        4.2   0.748     ...            0           0         0   \n",
       "1       3.6        5.1   0.709     ...            0           0         0   \n",
       "2       4.1        7.1   0.575     ...            0           0         0   \n",
       "3       4.8        7.6   0.627     ...            0           0         0   \n",
       "4       4.9        5.8   0.838     ...            0           0         0   \n",
       "\n",
       "   Arkansas  Memphis  Louisville  Indiana  LSU  Notre Dame  Ohio State  \n",
       "0         0        0           0        0    0           0           0  \n",
       "1         0        0           0        0    0           0           0  \n",
       "2         0        0           0        0    1           0           0  \n",
       "3         0        0           0        0    0           0           0  \n",
       "4         0        0           0        0    0           0           0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('/Users/ct/DSI-NYC-4/projects/capstone/part-03/nonulls_2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g</th>\n",
       "      <th>mp_per_g</th>\n",
       "      <th>fg_per_g</th>\n",
       "      <th>fga_per_g</th>\n",
       "      <th>fg_pct</th>\n",
       "      <th>ft_per_g</th>\n",
       "      <th>fta_per_g</th>\n",
       "      <th>ft_pct</th>\n",
       "      <th>trb_per_g</th>\n",
       "      <th>ast_per_g</th>\n",
       "      <th>...</th>\n",
       "      <th>Alabama</th>\n",
       "      <th>Georgetown</th>\n",
       "      <th>Syracuse</th>\n",
       "      <th>Arkansas</th>\n",
       "      <th>Memphis</th>\n",
       "      <th>Louisville</th>\n",
       "      <th>Indiana</th>\n",
       "      <th>LSU</th>\n",
       "      <th>Notre Dame</th>\n",
       "      <th>Ohio State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>29.599117</td>\n",
       "      <td>7.1</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0.540</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.748</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.623</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.709</td>\n",
       "      <td>10.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>8.7</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0.610</td>\n",
       "      <td>4.1</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.575</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>8.1</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0.613</td>\n",
       "      <td>4.8</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>4.4</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.472</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.838</td>\n",
       "      <td>3.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     g   mp_per_g  fg_per_g  fga_per_g  fg_pct  ft_per_g  fta_per_g  ft_pct  \\\n",
       "0  101  29.599117       7.1       13.2   0.540       3.1        4.2   0.748   \n",
       "1   40  32.000000       5.3        8.4   0.623       3.6        5.1   0.709   \n",
       "2   90  30.500000       8.7       14.3   0.610       4.1        7.1   0.575   \n",
       "3  127  29.500000       8.1       13.3   0.613       4.8        7.6   0.627   \n",
       "4   63  33.500000       4.4        9.3   0.472       4.9        5.8   0.838   \n",
       "\n",
       "   trb_per_g  ast_per_g     ...      Alabama  Georgetown  Syracuse  Arkansas  \\\n",
       "0        5.0        1.8     ...            0           0         0         0   \n",
       "1       10.4        1.3     ...            0           0         0         0   \n",
       "2       13.5        1.7     ...            0           0         0         0   \n",
       "3       10.3        0.7     ...            0           0         0         0   \n",
       "4        3.9        6.3     ...            0           0         0         0   \n",
       "\n",
       "   Memphis  Louisville  Indiana  LSU  Notre Dame  Ohio State  \n",
       "0        0           0        0    0           0           0  \n",
       "1        0           0        0    0           0           0  \n",
       "2        0           0        0    1           0           0  \n",
       "3        0           0        0    0           0           0  \n",
       "4        0           0        0    0           0           0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df.iloc[:,2:]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27.9\n",
       "1    26.6\n",
       "2    26.4\n",
       "3    26.2\n",
       "4    25.7\n",
       "Name: PER, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=df.iloc[:,0]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "X_scaled = X_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LinearRegression()\n",
    "lr.fit(X_scaled,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.127353160762028"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(lr,X_scaled,y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge=Ridge()\n",
    "ridge.fit(X_scaled,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13.052048063925648"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(ridge,X,y,cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, I tried a couple of different models for my data, both of which produced terrible results. I first tried a simple linear regression, but the cross validation score for the model was -6.1. I then tried out a Ridge regression, and it somehow came out worse with a cross val score of -13. I think that i'm probably going to need help with figuring out how to move forward. I can try to play with using different subsets of features, but I'm not sure how it would help. Also, I have considered possibly turning the target into some sort of categorical value to make it easier for my model to make predictions. \n",
    "\n",
    "Before moving forward, I think that I need to figure out exactly why my models are performing so bad and then address those issues if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb=GradientBoostingRegressor()\n",
    "gb.fit(X_scaled,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.1726514901579463"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(gb,X_scaled,y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
